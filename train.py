# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-jv1TQvdgeAS3d1K25GuN6pII5qVkCDq
"""

from utils import *
from layers import *
from models import *

train_dataset = eval(dataset_class)(
    path, 
    data, 
    split_edge, 
    num_hops=num_hops, 
    percent=100, 
    split='train', 
    use_coalesce=use_coalesce, 
    ratio_per_hop=1, 
    max_nodes_per_hop=None, 
    directed=False, 
) 

val_dataset = eval(dataset_class)(
    path, 
    data, 
    split_edge, 
    num_hops=num_hops, 
    percent=100, 
    split='valid', 
    use_coalesce=use_coalesce, 
    ratio_per_hop=1, 
    max_nodes_per_hop=None, 
    directed=False, 
) 

test_dataset = eval(dataset_class)(
    path, 
    data, 
    split_edge, 
    num_hops=num_hops, 
    percent=100, 
    split='test', 
    use_coalesce=use_coalesce, 
    ratio_per_hop=1, 
    max_nodes_per_hop=None, 
    directed=False, 
) 


max_z = 1000 
num_workers = 0
batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                          shuffle=True, num_workers=num_workers)
val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                        num_workers=num_workers)
test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                         num_workers=num_workers)

n_epochs = 50
lr = 0.0005
n_hid = 32
dropout = 0.6
seed = 123
param_var = 1
param_kl = 5e-4
param_l2 = 5e-4
early_stopping = 20
device = 'cuda'
in_feats = data.x.size()[1] + n_hid + n_hid
out_feats = 32

emb = torch.nn.Embedding(data.num_nodes, 32).to(device)
model = Robust_SEAL(in_feats, out_feats, n_hid, dropout, param_var, node_embedding = emb)
torch.nn.init.xavier_uniform_(emb.weight)
parameters = list(model.parameters())
parameters += list(emb.parameters())
optimizer = torch.optim.Adam(params=parameters, lr=lr)

def train():
  total_loss = 0
  model.train()
  
  for data in train_loader:
    if data.y.size()[0] >=batch_size:
        data = data.to(device)
        adj = torch_geometric.utils.to_dense_adj(edge_index = data.edge_index, edge_attr = data.edge_weight, max_num_nodes = data.num_nodes)[0].cuda()
        features = data.x
        features = preprocess_features_tensor(features)
        adj_list = [
        preprocess_adj_tensor(adj, -0.5),
        preprocess_adj_tensor(adj, -1.0)
        ]
        logits = model(adj_list, features, data, training = True).cuda()

        optimizer.zero_grad()
        
        loss = BCEWithLogitsLoss()(logits.view(-1), data.y.to(torch.float))
        loss += param_l2 * torch.norm(model.layers[0].weight)
        mean = model.layers[0].mean
        var = model.layers[0].var
        KL_divergence = 0.5 * torch.mean(torch.square(mean) + var - torch.log(1e-8 + var) - 1, 1)
        KL_divergence = torch.sum(KL_divergence)
        loss += param_kl * KL_divergence

        loss.backward()
        optimizer.step()     
        total_loss += loss.item() * data.num_graphs

  return total_loss / len(train_dataset)


def test(eval_metric):
  model.eval()

  total_loss = 0

  y_pred, y_true, y_src, y_dst, y_weight = [], [], [], [], []
  for data in val_loader:
    if data.y.size()[0] >=batch_size:
        data = data.to(device)
        adj = torch_geometric.utils.to_dense_adj(edge_index = data.edge_index, edge_attr = data.edge_weight, max_num_nodes = data.num_nodes)[0].cuda()
        features = data.x
        features = preprocess_features_tensor(features)
        adj_list = [
        preprocess_adj_tensor(adj, -0.5),
        preprocess_adj_tensor(adj, -1.0)
        ]
        logits = model(adj_list, features, data, training = False).cuda()
        
        loss = BCEWithLogitsLoss()(logits.view(-1), data.y.to(torch.float))
        loss += param_l2 * torch.norm(model.layers[0].weight)
        mean = model.layers[0].mean
        var = model.layers[0].var
        KL_divergence = 0.5 * torch.mean(torch.square(mean) + var - torch.log(1e-8 + var) - 1, 1)
        KL_divergence = torch.sum(KL_divergence)
        loss += param_kl * KL_divergence

        y_pred.append(logits.view(-1).cpu())
        y_true.append(data.y.view(-1).cpu().to(torch.float))
        y_src.append(torch.tensor(data.target).view(2,-1)[0])
        y_dst.append(torch.tensor(data.target).view(2,-1)[1])
        y_weight.append(torch.tensor(data.target_weight).view(-1).cpu())
        total_loss += loss.item() * data.num_graphs
    
  val_pred, val_true, val_src, val_dst, val_weight = torch.cat(y_pred), torch.cat(y_true), torch.cat(y_src), torch.cat(y_dst), torch.cat(y_weight)


  y_pred, y_true, y_src, y_dst, y_weight = [], [], [], [], []
  for data in test_loader:
    if data.y.size()[0] >=batch_size:
        data = data.to(device)
        adj = torch_geometric.utils.to_dense_adj(edge_index = data.edge_index, edge_attr = data.edge_weight, max_num_nodes = data.num_nodes)[0].cuda()
        features = data.x
        features = preprocess_features_tensor(features)
        adj_list = [
        preprocess_adj_tensor(adj, -0.5),
        preprocess_adj_tensor(adj, -1.0)
        ]
        logits = model(adj_list, features, data, training = False).cuda()

        y_pred.append(logits.view(-1).cpu())
        y_true.append(data.y.view(-1).cpu().to(torch.float))
        y_src.append(torch.tensor(data.target).view(2,-1)[0])
        y_dst.append(torch.tensor(data.target).view(2,-1)[1])
        y_weight.append(torch.tensor(data.target_weight).view(-1).cpu())
    
  test_pred, test_true, test_src, test_dst, test_weight = torch.cat(y_pred), torch.cat(y_true), torch.cat(y_src), torch.cat(y_dst), torch.cat(y_weight)

  if 'ndcg' in eval_metric :
    val_result, test_result = evaluate_ndcg(val_pred, val_src, val_dst, val_weight, test_pred, test_src, test_dst, test_weight, eval_metric)
    return val_result, test_result, total_loss / len(val_dataset)

loss_list = []
for epoch in range(1, 201):
    loss = train()
    val_ndcg, test_ndcg, val_loss = test('ndcg')
    if epoch > early_stopping and val_loss > np.mean(loss_list[-early_stopping:]):
      print("---Early stopping---")
      break
    loss_list.append(val_loss)
    print(f'E: {epoch:02d}, T_Loss: {loss:.4f}, V_Loss: {val_loss:.4f}, '
    f'V_ndcg: {val_ndcg}, T_ndcg: {test_ndcg}')