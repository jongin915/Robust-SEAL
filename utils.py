# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fgEd-SxhsA7L-aTSEP92jUQPKc4SBbgA
"""

import os
import time
import random
import numpy as np
import torch
import torch.nn as nn
import dgl
import dgl.function as fn

def split_nodes(data, val_ratio = 0.05, test_ratio = 0.1):
    num_nodes = data.num_nodes()

    n_v = int(math.floor(val_ratio * num_nodes))
    n_t = int(math.floor(test_ratio * num_nodes))

    perm = torch.randperm(num_nodes)
    val_nodes = perm[:n_v]
    test_nodes = perm[n_v:n_v+n_t]
    train_nodes = perm[n_v+n_t:]

    return train_nodes, val_nodes, test_nodes

def create_dataset_inductive(data, train_nodes, val_nodes, test_nodes, edge_weight):

    num_nodes = data.num_nodes()
    row, col = data.edges()

    val_mask1 = torch.tensor([True if int(x) in val_nodes else False for x in row]) * torch.tensor([True if int(x) in val_nodes else False for x in col])
    val_mask2 = torch.tensor([True if int(x) in train_nodes else False for x in row]) * torch.tensor([True if int(x) in val_nodes else False for x in col])
    val_mask3 = torch.tensor([True if int(x) in val_nodes else False for x in row]) * torch.tensor([True if int(x) in train_nodes else False for x in col])
    val_mask = val_mask1+val_mask2+val_mask3

    test_mask1 = torch.tensor([True if int(x) in test_nodes else False for x in row]) * torch.tensor([True if int(x) in test_nodes else False for x in col])
    test_mask2 = torch.tensor([True if int(x) in train_nodes else False for x in row]) * torch.tensor([True if int(x) in test_nodes else False for x in col])
    test_mask3 = torch.tensor([True if int(x) in test_nodes else False for x in row]) * torch.tensor([True if int(x) in train_nodes else False for x in col])
    test_mask = test_mask1+test_mask2+test_mask3

    train_mask = torch.tensor([True if int(x) in train_nodes else False for x in row]) * torch.tensor([True if int(x) in train_nodes else False for x in col])

    val_pos_u, val_pos_v = row[val_mask], col[val_mask]
    val_pos_weight = edge_weight[val_mask]

    test_pos_u, test_pos_v = row[test_mask], col[test_mask]
    test_pos_weight = edge_weight[test_mask]

    train_pos_u, train_pos_v = row[train_mask], col[train_mask]
    train_pos_weight= edge_weight[train_mask]

    # Negative edges.
    neg_adj_mask = torch.ones(num_nodes, num_nodes, dtype=torch.uint8)
    neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)
    neg_adj_mask[row, col] = 0

    neg_row, neg_col = neg_adj_mask.nonzero(as_tuple=False).t()


    train_neg_mask = torch.tensor([True if int(x) in train_nodes else False for x in neg_row]) * torch.tensor([True if int(x) in train_nodes else False for x in neg_col])

    train_neg_row, train_neg_col = neg_row[train_neg_mask], neg_col[train_neg_mask]
    perm = torch.randperm(train_neg_row.size(0))[:torch.stack([train_pos_u, train_pos_v], dim=0).size()[1]]
    train_neg_u, train_neg_v = train_neg_row[perm], train_neg_col[perm]


    val_neg_mask1 = torch.tensor([True if int(x) in val_nodes else False for x in neg_row]) * torch.tensor([True if int(x) in val_nodes else False for x in neg_col])
    val_neg_mask2 = torch.tensor([True if int(x) in val_nodes else False for x in neg_row]) * torch.tensor([True if int(x) in train_nodes else False for x in neg_col])
    val_neg_mask3 = torch.tensor([True if int(x) in train_nodes else False for x in neg_row]) * torch.tensor([True if int(x) in val_nodes else False for x in neg_col])
    val_neg_mask = val_neg_mask1 + val_neg_mask2 + val_neg_mask3
      # val_neg_mask.append(val_neg_mask2+val_neg_mask3)

    val_neg_row, val_neg_col = neg_row[val_neg_mask], neg_col[val_neg_mask]
    perm = torch.randperm(val_neg_row.size(0))[:torch.stack([val_pos_u, val_pos_v], dim=0).size()[1]]
    val_neg_u, val_neg_v = val_neg_row[perm], val_neg_col[perm]

    test_neg_mask1 = torch.tensor([True if int(x) in test_nodes else False for x in neg_row]) * torch.tensor([True if int(x) in test_nodes else False for x in neg_col])
    test_neg_mask2 = torch.tensor([True if int(x) in test_nodes else False for x in neg_row]) * torch.tensor([True if int(x) in train_nodes else False for x in neg_col])
    test_neg_mask3 = torch.tensor([True if int(x) in train_nodes else False for x in neg_row]) * torch.tensor([True if int(x) in test_nodes else False for x in neg_col])
    test_neg_mask = val_neg_mask1 + val_neg_mask2 + val_neg_mask3
      # test_neg_mask.append(test_neg_mask2+test_neg_mask3)

    test_neg_row, test_neg_col = neg_row[test_neg_mask], neg_col[test_neg_mask]
    perm = torch.randperm(test_neg_row.size(0))[:torch.stack([test_pos_u, test_pos_v], dim=0).size()[1]]
    test_neg_u, test_neg_v = test_neg_row[perm], test_neg_col[perm]


    return train_pos_u, train_pos_v, train_pos_weight, val_pos_u, val_pos_v, val_pos_weight, test_pos_u, test_pos_v, test_pos_weight, train_neg_u, train_neg_v, val_neg_u, val_neg_v, test_neg_u, test_neg_v


def split_edges_inductive(train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g, split_ratio = 0.1):
  num_nodes = train_pos_g.number_of_nodes()
  n_val = int(len(val_pos_g.edges()[0])*split_ratio)
  n_test = int(len(test_pos_g.edges()[0])*split_ratio)
      
  perm = torch.randperm(len(val_pos_g.edges()[0]))
  val_train, val_test = perm[n_val:], perm[:n_val]
  perm = torch.randperm(len(test_pos_g.edges()[0]))
  test_train, test_test = perm[n_test:], perm[:n_test]


  # split val data
  val_train_pos_u = torch.cat((val_pos_g.edges()[0][val_train], train_pos_u), dim = 0)
  val_train_pos_v = torch.cat((val_pos_g.edges()[1][val_train], train_pos_v), dim = 0)
  val_train_pos_weight = torch.cat((val_pos_g.edata['w'][val_train], train_pos_weight), dim = 0)

  val_train_neg_u = torch.cat((val_neg_g.edges()[0][val_train], train_neg_u), dim = 0)
  val_train_neg_v = torch.cat((val_neg_g.edges()[1][val_train], train_neg_v), dim = 0)
  val_train_neg_weight = torch.zeros(val_train_neg_u.size()[0])

  val_test_pos_u = val_pos_g.edges()[0][val_test]
  val_test_pos_v = val_pos_g.edges()[1][val_test]
  val_test_pos_weight = val_pos_g.edata['w'][val_test]

  val_test_neg_u = val_neg_g.edges()[0][val_test]
  val_test_neg_v = val_neg_g.edges()[1][val_test]
  val_test_neg_weight = torch.zeros(val_test_neg_u.size()[0])

  # split test data
  test_train_pos_u = torch.cat((test_pos_g.edges()[0][test_train], train_pos_u), dim = 0)
  test_train_pos_v = torch.cat((test_pos_g.edges()[1][test_train], train_pos_v), dim = 0)
  test_train_pos_weight = torch.cat((test_pos_g.edata['w'][test_train], train_pos_weight), dim = 0)

  test_train_neg_u = torch.cat((test_neg_g.edges()[0][test_train], train_neg_u), dim = 0)
  test_train_neg_v = torch.cat((test_neg_g.edges()[1][test_train], train_neg_v), dim = 0)
  test_train_neg_weight = torch.zeros(test_train_neg_u.size()[0])

  test_test_pos_u = test_pos_g.edges()[0][test_test]
  test_test_pos_v = test_pos_g.edges()[1][test_test]
  test_test_pos_weight = test_pos_g.edata['w'][test_test]

  test_test_neg_u = test_neg_g.edges()[0][test_test]
  test_test_neg_v = test_neg_g.edges()[1][test_test]
  test_test_neg_weight = torch.zeros(test_test_neg_u.size()[0])

  # create train graphs
  train_u = torch.cat((train_pos_u, train_neg_u), dim = 0)
  train_v = torch.cat((train_pos_v, train_neg_v), dim = 0)
  train_weight = torch.cat((train_pos_weight, torch.zeros(train_neg_u.size()[0])), dim = 0)
  train_g = dgl.graph((train_u, train_v), num_nodes=g.number_of_nodes())
  train_g.edata['w'] = train_weight

  # create valid graphs
  val_train_u = torch.cat((val_train_pos_u, val_train_neg_u), dim = 0)
  val_train_v = torch.cat((val_train_pos_v, val_train_neg_v), dim = 0)
  val_train_weight = torch.cat((val_train_pos_weight, torch.zeros(val_train_neg_u.size()[0])), dim = 0)
  val_train_g = dgl.graph((val_train_u, val_train_v), num_nodes=g.number_of_nodes())
  val_train_g.edata['w'] = val_train_weight
  val_train_pos_g = dgl.graph((val_train_pos_u, val_train_pos_v), num_nodes=num_nodes)
  val_train_pos_g.edata['w'] = val_train_pos_weight

  val_test_pos_g = dgl.graph((val_test_pos_u, val_test_pos_v), num_nodes=num_nodes)
  val_test_pos_g.edata['w'] = val_test_pos_weight

  val_test_neg_g = dgl.graph((val_test_neg_u, val_test_neg_v), num_nodes=num_nodes)

  # create test graphs
  test_train_u = torch.cat((test_train_pos_u, test_train_neg_u), dim = 0)
  test_train_v = torch.cat((test_train_pos_v, test_train_neg_v), dim = 0)
  test_train_weight = torch.cat((test_train_pos_weight, torch.zeros(test_train_neg_u.size()[0])), dim = 0)
  test_train_g = dgl.graph((test_train_u, test_train_v), num_nodes=g.number_of_nodes())
  test_train_g.edata['w'] = test_train_weight
  test_train_pos_g = dgl.graph((test_train_pos_u, test_train_pos_v), num_nodes=num_nodes)
  test_train_pos_g.edata['w'] = test_train_pos_weight

  test_test_pos_g = dgl.graph((test_test_pos_u, test_test_pos_v), num_nodes=num_nodes)
  test_test_pos_g.edata['w'] = test_test_pos_weight

  test_test_neg_g = dgl.graph((test_test_neg_u, test_test_neg_v), num_nodes=num_nodes)

  return train_g, val_train_g, val_test_pos_g, val_train_pos_g, val_test_neg_g, test_train_g, test_train_pos_g, test_test_pos_g, test_test_neg_g

def neighbors(fringe, A, outgoing=True):
    if outgoing:
        res = set(A[list(fringe)].indices)
    else:
        res = set(A[:, list(fringe)].indices)

    return res

def k_hop_subgraph(src, dst, num_hops, A, sample_ratio=1.0, 
                   max_nodes_per_hop=None, node_features=None, 
                   y=None, directed=False, A_csc=None):
    # Extract the k-hop enclosing subgraph around link (src, dst) from A. 
    nodes = [src, dst]
    if src> dst:
      target = [src, dst]
    else :
      target = [dst, src]
    dists = [0, 0]
    visited = set([src, dst])
    fringe = set([src, dst])
    for dist in range(1, num_hops+1):
        if not directed:
            fringe = neighbors(fringe, A)
        else:
            out_neighbors = neighbors(fringe, A)
            in_neighbors = neighbors(fringe, A_csc, False)
            fringe = out_neighbors.union(in_neighbors)
        fringe = fringe - visited
        visited = visited.union(fringe)
        if sample_ratio < 1.0:
            fringe = random.sample(fringe, int(sample_ratio*len(fringe)))
        if max_nodes_per_hop is not None:
            if max_nodes_per_hop < len(fringe):
                fringe = random.sample(fringe, max_nodes_per_hop)
        if len(fringe) == 0:
            break
        nodes = nodes + list(fringe)
        dists = dists + [dist] * len(fringe)
    subgraph = A[nodes, :][:, nodes]

    # Remove target link between the subgraph.
    target_weight = subgraph[0,1]
    subgraph[0, 1] = 0
    subgraph[1, 0] = 0

    if node_features is not None:
        node_features = node_features[nodes]

    return nodes, subgraph, dists, node_features, y, target, target_weight


def drnl_node_labeling(adj, src, dst):
    # Double Radius Node Labeling (DRNL).
    src, dst = (dst, src) if src > dst else (src, dst)

    idx = list(range(src)) + list(range(src + 1, adj.shape[0]))
    adj_wo_src = adj[idx, :][:, idx]

    idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))
    adj_wo_dst = adj[idx, :][:, idx]

    dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=False, indices=src)
    dist2src = np.insert(dist2src, dst, 0, axis=0)
    dist2src = torch.from_numpy(dist2src)

    dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=False, indices=dst-1)
    dist2dst = np.insert(dist2dst, src, 0, axis=0)
    dist2dst = torch.from_numpy(dist2dst)

    dist = dist2src + dist2dst
    dist_over_2, dist_mod_2 = dist // 2, dist % 2

    z = 1 + torch.min(dist2src, dist2dst)
    z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)
    z[src] = 1.
    z[dst] = 1.
    z[torch.isnan(z)] = 0.

    return z.to(torch.long)

def get_pos_neg_edges(split, split_edge, edge_index, num_nodes, percent=100):
    if 'edge' in split_edge['train']:
        pos_edge = split_edge[split]['edge'].t()
        if split == 'train':
            new_edge_index, _ = add_self_loops(edge_index)
            neg_edge = negative_sampling(
                new_edge_index, num_nodes=num_nodes,
                num_neg_samples=pos_edge.size(1))
        else:
            neg_edge = split_edge[split]['edge_neg'].t()
        # subsample for pos_edge
        np.random.seed(123)
        num_pos = pos_edge.size(1)
        perm = np.random.permutation(num_pos)
        perm = perm[:int(percent / 100 * num_pos)]
        pos_edge = pos_edge[:, perm]
        # subsample for neg_edge
        np.random.seed(123)
        num_neg = neg_edge.size(1)
        perm = np.random.permutation(num_neg)
        perm = perm[:int(percent / 100 * num_neg)]
        neg_edge = neg_edge[:, perm]

    elif 'source_node' in split_edge['train']:
        source = split_edge[split]['source_node']
        target = split_edge[split]['target_node']
        if split == 'train':
            target_neg = torch.randint(0, num_nodes, [target.size(0), 1],
                                       dtype=torch.long)
        else:
            target_neg = split_edge[split]['target_node_neg']
        # subsample
        np.random.seed(123)
        num_source = source.size(0)
        perm = np.random.permutation(num_source)
        perm = perm[:int(percent / 100 * num_source)]
        source, target, target_neg = source[perm], target[perm], target_neg[perm, :]
        pos_edge = torch.stack([source, target])
        neg_per_target = target_neg.size(1)
        neg_edge = torch.stack([source.repeat_interleave(neg_per_target), 
                                target_neg.view(-1)])
    return pos_edge, neg_edge

def extract_enclosing_subgraphs(link_index, A, x, y, num_hops, 
                                ratio_per_hop=1.0, max_nodes_per_hop=None, 
                                directed=False, A_csc=None):
    # Extract enclosing subgraphs from A for all links in link_index.
    data_list = []
    for src, dst in tqdm(link_index.t().tolist()):
        k_nodes, k_subgraph, k_dists, k_node_features, k_y, k_target, k_target_weight = k_hop_subgraph(src, dst, num_hops, A, ratio_per_hop, 
                             max_nodes_per_hop, node_features=x, y=y, directed=directed, A_csc=A_csc)
        data = construct_pyg_graph(k_nodes, k_subgraph, k_dists, k_node_features, k_y, k_target, k_target_weight)
        data_list.append(data)

    return data_list

def k_hop_subgraph(src, dst, num_hops, A, sample_ratio=1.0, 
                   max_nodes_per_hop=None, node_features=None, 
                   y=None, directed=False, A_csc=None):
    # Extract the k-hop enclosing subgraph around link (src, dst) from A. 
    nodes = set([src, dst])
    if src> dst:
      target = [src, dst]
    else :
      target = [dst, src]
    dists = [0, 0]
    visited = set([src, dst])
    fringe = set([src, dst])
    for dist in range(1, num_hops+1):
        if not directed:
            fringe = neighbors(fringe, A)
        else:
            out_neighbors = neighbors(fringe, A)
            in_neighbors = neighbors(fringe, A_csc, False)
            fringe = out_neighbors.union(in_neighbors)
        fringe = fringe - visited
        visited = visited.union(fringe)
        if sample_ratio < 1.0:
            fringe = random.sample(fringe, int(sample_ratio*len(fringe)))
        if max_nodes_per_hop is not None:
            if max_nodes_per_hop < len(fringe):
                fringe = random.sample(fringe, max_nodes_per_hop)
        if len(fringe) == 0:
            break
        nodes = nodes.union(fringe)
        dists = dists + [dist] * len(fringe)
    
    nodes.remove(src)
    nodes.remove(dst)
    nodes = [src, dst] + list(nodes) 
    subgraph = A[nodes, :][:, nodes]

    # Remove target link between the subgraph.
    target_weight = subgraph[0,1]+subgraph[1,0]
    subgraph[0, 1] = 0
    subgraph[1, 0] = 0

    if node_features is not None:
        node_features = node_features[nodes]

    return nodes, subgraph, dists, node_features, y, target, target_weight

def construct_pyg_graph(node_ids, adj, dists, node_features, y, target=None, target_weight=None):
    # Construct a pytorch_geometric graph from a scipy csr adjacency matrix.
    u, v, r = ssp.find(adj)
    num_nodes = adj.shape[0]
    
    node_ids = torch.LongTensor(node_ids)
    u, v = torch.LongTensor(u), torch.LongTensor(v)
    r = torch.LongTensor(r)
    edge_index = torch.stack([u, v], 0)
    edge_weight = r.to(torch.float)
    y = torch.tensor([y])
    z = drnl_node_labeling(adj, 0, 1)

    data = Data(node_features, edge_index, edge_weight=edge_weight, y=y, z=z, 
                node_id=node_ids, num_nodes=num_nodes, target=target, target_weight = target_weight)
    return data

class SEALDataset(InMemoryDataset):
    def __init__(self, root, data, split_edge, num_hops, percent=100, split='train', 
                 use_coalesce=False, ratio_per_hop=1.0, 
                 max_nodes_per_hop=None, directed=False):
        self.data = data
        self.split_edge = split_edge
        self.num_hops = num_hops
        self.percent = int(percent) if percent >= 1.0 else percent
        self.split = split
        self.use_coalesce = use_coalesce
        self.ratio_per_hop = ratio_per_hop
        self.max_nodes_per_hop = max_nodes_per_hop
        self.directed = directed
        super(SEALDataset, self).__init__(root)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def processed_file_names(self):
        if self.percent == 100:
            name = 'SEAL_{}_data'.format(self.split)
        else:
            name = 'SEAL_{}_data_{}'.format(self.split, self.percent)
        name += '.pt'
        return [name]

    def process(self):
        pos_edge, neg_edge = get_pos_neg_edges(self.split, self.split_edge, 
                                               self.data.edge_index, 
                                               self.data.num_nodes, 
                                               self.percent)

        if self.use_coalesce:  # compress mutli-edge into edge with weight
            self.data.edge_index, self.data.edge_weight = coalesce(
                self.data.edge_index, self.data.edge_weight, 
                self.data.num_nodes, self.data.num_nodes)

        edge_weight = self.data.edge_attr.view(-1)

        A = ssp.csr_matrix(
            (edge_weight, (self.data.edge_index[0], self.data.edge_index[1])), 
            shape=(self.data.num_nodes, self.data.num_nodes)
        )

        if self.directed:
            A_csc = A.tocsc()
        else:
            A_csc = None
        
        # Extract enclosing subgraphs for pos and neg edges
        pos_list = extract_enclosing_subgraphs(
            pos_edge, A, self.data.x, 1, self.num_hops,
            self.ratio_per_hop, self.max_nodes_per_hop, self.directed, A_csc)
        neg_list = extract_enclosing_subgraphs(
            neg_edge, A, self.data.x, 0, self.num_hops,
            self.ratio_per_hop, self.max_nodes_per_hop, self.directed, A_csc)

        torch.save(self.collate(pos_list + neg_list), self.processed_paths[0])
        del pos_list, neg_list

def evaluate_ndcg(val_pred, val_src, val_dst, val_weight, test_pred, test_src, test_dst, test_weight, metric):
    
    val_scores_1, val_scores_2, val_scores_3, val_scores_4, val_scores_5 = [], [], [], [], []
    val_node = torch.cat((val_src, val_dst),0).tolist()
    for node in set(val_node):
      src_mask = (node == val_src)
      dst_mask = (node == val_src)

      pred_mask = torch.cat((val_pred[src_mask], val_pred[dst_mask]), 0)
      weight_mask = torch.cat((val_weight[src_mask], val_weight[dst_mask]), 0)
      if pred_mask.size()[0] != 0:

        pred = weight_mask[pred_mask.sort(descending = True)[1]]
        score = weight_mask.sort(descending = True)[0]

        pred_array = np.asarray([pred.tolist()])
        score_array = np.asarray([score.tolist()])

        if len(pred_array[0]) >= 1 :
          val_score_1 = ndcg_score(pred_array, score_array, k=1, ignore_ties = True)
          val_scores_1.append(val_score_1)
        
        if len(pred_array[0]) >= 2 :
          val_score_2 = ndcg_score(pred_array, score_array, k=2, ignore_ties = True)
          val_scores_2.append(val_score_2)

        if len(pred_array[0]) >= 3 :
          val_score_3 = ndcg_score(pred_array, score_array, k=3, ignore_ties = True)
          val_scores_3.append(val_score_3)
        
        if len(pred_array[0]) >= 4 :
          val_score_4 = ndcg_score(pred_array, score_array, k=4, ignore_ties = True)
          val_scores_4.append(val_score_4)

        if len(pred_array[0]) >= 5 :
          val_score_5 = ndcg_score(pred_array, score_array, k=5, ignore_ties = True)
          val_scores_5.append(val_score_5)
      
    val_result_1 = round(sum(val_scores_1) / len(val_scores_1),4)
    val_result_2 = round(sum(val_scores_2) / len(val_scores_2),4)
    val_result_3 = round(sum(val_scores_3) / len(val_scores_3),4)
    val_result_4 = round(sum(val_scores_4) / len(val_scores_4),4)
    val_result_5 = round(sum(val_scores_5) / len(val_scores_5),4)


    test_scores_1, test_scores_2, test_scores_3, test_scores_4, test_scores_5 = [], [], [], [], []
    test_node = torch.cat((test_src, test_dst),0).tolist()
    for node in set(test_node):
      src_mask = (node == test_src)
      dst_mask = (node == test_dst)

      pred_mask = torch.cat((test_pred[src_mask], test_pred[dst_mask]), 0)
      weight_mask = torch.cat((test_weight[src_mask], test_weight[dst_mask]), 0)
      if pred_mask.size()[0] != 0:

        pred = weight_mask[pred_mask.sort(descending = True)[1]]
        score = weight_mask.sort(descending = True)[0]

        pred_array = np.asarray([pred.tolist()])
        score_array = np.asarray([score.tolist()])
        
        if len(pred_array[0]) >= 1 :
          test_score_1 = ndcg_score(pred_array, score_array, k=1, ignore_ties = True)
          test_scores_1.append(test_score_1)
        
        if len(pred_array[0]) >= 2 :
          test_score_2 = ndcg_score(pred_array, score_array, k=2, ignore_ties = True)
          test_scores_2.append(test_score_2)

        if len(pred_array[0]) >= 3 :
          test_score_3 = ndcg_score(pred_array, score_array, k=3, ignore_ties = True)
          test_scores_3.append(test_score_3)
        
        if len(pred_array[0]) >= 4 :
          test_score_4 = ndcg_score(pred_array, score_array, k=4, ignore_ties = True)
          test_scores_4.append(test_score_4)

        if len(pred_array[0]) >= 5 :
          test_score_5 = ndcg_score(pred_array, score_array, k=5, ignore_ties = True)
          test_scores_5.append(test_score_5)
      
    test_result_1 = round(sum(test_scores_1) / len(test_scores_1),4)
    test_result_2 = round(sum(test_scores_2) / len(test_scores_2),4)
    test_result_3 = round(sum(test_scores_3) / len(test_scores_3),4)
    test_result_4 = round(sum(test_scores_4) / len(test_scores_4),4)
    test_result_5 = round(sum(test_scores_5) / len(test_scores_5),4)

    val_result = [val_result_1,val_result_2,val_result_3,val_result_4,val_result_5]
    test_result = [test_result_1,test_result_2,test_result_3,test_result_4,test_result_5]
    return val_result, test_result

def sparse_to_tuple(sparse_mx):
    """Convert sparse matrix to tuple representation."""
    def to_tuple(mx):
        if not ssp.isspmatrix_coo(mx):
            mx = mx.tocoo()
        coords = np.vstack((mx.row, mx.col)).transpose()
        values = mx.data
        shape = mx.shape
        return coords, values, shape

    if isinstance(sparse_mx, list):
        for i in range(len(sparse_mx)):
            sparse_mx[i] = to_tuple(sparse_mx[i])
    else:
        sparse_mx = to_tuple(sparse_mx)

    return sparse_mx


def preprocess_features(features):
    """Row-normalize feature matrix and convert to tuple representation"""
    rowsum = np.array(features.sum(1))
    r_inv = np.power(rowsum, -1).flatten()
    r_inv[np.isinf(r_inv)] = 0.
    r_mat_inv = ssp.diags(r_inv)
    features = r_mat_inv.dot(features)
    return sparse_to_tuple(features)

def preprocess_features_tensor(features):
    """Row-normalize feature matrix and convert to tuple representation"""
    rowsum = features.sum(1,True)
    r_inv = torch.pow(torch.transpose(rowsum,1,0),-1)
    r_inv[torch.isinf(r_inv)] = 0.
    r_mat_inv = torch.diag(r_inv[0])
    features = r_mat_inv.matmul(features)
    return features.to_sparse(2)


def normalize_adj(adj, alpha):
    """Symmetrically normalize adjacency matrix."""
    adj = ssp.coo_matrix(adj)
    rowsum = np.array(adj.sum(1))
    d_inv_sqrt = np.power(rowsum, alpha).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = ssp.diags(d_inv_sqrt)
    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()

""" torch version"""
def normalize_adj_tensor(adj, alpha):
    rowsum = adj.sum(1,True)
    d_inv_sqrt = torch.pow(torch.transpose(rowsum,1,0),alpha)
    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = torch.diag(d_inv_sqrt[0])
    return adj.matmul(d_mat_inv_sqrt).transpose(0,1).matmul(d_mat_inv_sqrt).to_sparse(2)

def preprocess_adj_tensor(adj, alpha):
    """Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation."""
    adj_normalized = normalize_adj_tensor(adj + torch.eye(adj.shape[0]).cuda(), alpha)
    return adj_normalized


def preprocess_adj(adj, alpha):
    """Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation."""
    adj_normalized = normalize_adj(adj + ssp.eye(adj.shape[0]), alpha)
    return sparse_to_tuple(adj_normalized)


def tuple_to_tensor(input):
    """Convert tuple representation to torch sparse tensor."""
    return torch.sparse.FloatTensor(torch.LongTensor(input[0].T), torch.FloatTensor(input[1]), torch.Size(input[2]))


def fix_seed(seed=0):
    """Set random seed."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def uniform(shape, scale=0.05):
    """Uniform init."""
    initial = torch.rand(shape, dtype=torch.float32) * 2 * scale - scale
    return nn.Parameter(initial)


def glorot(input_dim, output_dim):
    """Create a weight variable with Glorot & Bengio (AISTATS 2010) initialization.
    """
    init_range = np.sqrt(6.0 / (input_dim + output_dim))
    initial = torch.rand(input_dim, output_dim, dtype=torch.float32) * 2 * init_range - init_range
    return nn.Parameter(initial)


def zeros(shape):
    """All zeros."""
    initial = torch.zeros(shape, dtype=torch.float32)
    return nn.Parameter(initial)


class DotPredictor(nn.Module):
    def forward(self, g, h):
        with g.local_scope():
            g.ndata['h'] = h
            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))
            return g.edata['score'][:, 0]


# The base class of sampler
# (TODO): online sampling
class SAINTSampler(object):
    def __init__(self, dn, g, train_nid, node_budget, num_repeat=50):
        """
        :param dn: name of dataset
        :param g: full graph
        :param train_nid: ids of training nodes
        :param node_budget: expected number of sampled nodes
        :param num_repeat: number of times of repeating sampling one node
        """
        self.g = g
        self.train_g: dgl.graph = g.subgraph(train_nid)
        self.dn, self.num_repeat = dn, num_repeat
        self.node_counter = torch.zeros((self.train_g.num_nodes(),))
        self.edge_counter = torch.zeros((self.train_g.num_edges(),))
        self.prob = None

        self.subgraphs = []
        self.N, sampled_nodes = 0, 0

        t = time.perf_counter()
        while sampled_nodes <= self.train_g.num_nodes() * num_repeat:
            subgraph = self.__sample__()
            self.subgraphs.append(subgraph)
            sampled_nodes += subgraph.shape[0]
            self.N += 1
        print(f'Sampling time: [{time.perf_counter() - t:.2f}s]')
        t = time.perf_counter()
        self.__counter__()
        aggr_norm, loss_norm = self.__compute_norm__()
        print(f'Normalization time: [{time.perf_counter() - t:.2f}s]')

        self.train_g.ndata['l_n'] = torch.Tensor(loss_norm)
        self.train_g.edata['w'] = torch.Tensor(aggr_norm)
        self.__compute_degree_norm()

        self.num_batch = math.ceil(self.train_g.num_nodes() / node_budget)
        random.shuffle(self.subgraphs)
        self.__clear__()
        print("The number of subgraphs is: ", len(self.subgraphs))
        print("The size of subgraphs is about: ", len(self.subgraphs[-1]))

    def __clear__(self):
        self.prob = None
        self.node_counter = None
        self.edge_counter = None
        self.g = None

    def __counter__(self):

        for sampled_nodes in self.subgraphs:
            sampled_nodes = torch.from_numpy(sampled_nodes)
            self.node_counter[sampled_nodes] += 1

            subg = self.train_g.subgraph(sampled_nodes)
            sampled_edges = subg.edata[dgl.EID]
            self.edge_counter[sampled_edges] += 1

    def __generate_fn__(self):
        raise NotImplementedError

    def __compute_norm__(self):
        self.node_counter[self.node_counter == 0] = 1
        self.edge_counter[self.edge_counter == 0] = 1

        loss_norm = self.N / self.node_counter / self.train_g.num_nodes()

        self.train_g.ndata['n_c'] = self.node_counter
        self.train_g.edata['e_c'] = self.edge_counter
        self.train_g.apply_edges(fn.v_div_e('n_c', 'e_c', 'a_n'))
        aggr_norm = self.train_g.edata.pop('a_n')

        self.train_g.ndata.pop('n_c')
        self.train_g.edata.pop('e_c')

        return aggr_norm.numpy(), loss_norm.numpy()

    def __compute_degree_norm(self):

        self.train_g.ndata['train_D_norm'] = 1. / self.train_g.in_degrees().float().clamp(min=1).unsqueeze(1)
        self.g.ndata['full_D_norm'] = 1. / self.g.in_degrees().float().clamp(min=1).unsqueeze(1)

    def __sample__(self):
        raise NotImplementedError

    def __len__(self):
        return self.num_batch

    def __iter__(self):
        self.n = 0
        return self

    def __next__(self):
        if self.n < self.num_batch:
            result = self.train_g.subgraph(self.subgraphs[self.n])
            self.n += 1
            return result
        else:
            random.shuffle(self.subgraphs)
            raise StopIteration()


class SAINTNodeSampler(SAINTSampler):
    def __init__(self, node_budget, dn, g, train_nid, num_repeat=50):
        self.node_budget = node_budget
        super(SAINTNodeSampler, self).__init__(dn, g, train_nid, node_budget, num_repeat)

    def __sample__(self):
        if self.prob is None:
            self.prob = self.train_g.in_degrees().float().clamp(min=1)

        sampled_nodes = torch.multinomial(self.prob, num_samples=self.node_budget, replacement=True).unique()
        return sampled_nodes.numpy()


class SAINTEdgeSampler(SAINTSampler):
    def __init__(self, edge_budget, dn, g, train_nid, num_repeat=50):
        self.edge_budget = edge_budget
        super(SAINTEdgeSampler, self).__init__(dn, g, train_nid, edge_budget * 2, num_repeat)

    def __sample__(self):
        if self.prob is None:
            src, dst = self.train_g.edges()
            src_degrees, dst_degrees = self.train_g.in_degrees(src).float().clamp(min=1),\
                                       self.train_g.in_degrees(dst).float().clamp(min=1)
            self.prob = 1. / src_degrees + 1. / dst_degrees

        sampled_edges = torch.multinomial(self.prob, num_samples=self.edge_budget, replacement=True).unique()

        sampled_src, sampled_dst = self.train_g.find_edges(sampled_edges)
        sampled_nodes = torch.cat([sampled_src, sampled_dst]).unique()
        return sampled_nodes.numpy()

def sparse_dropout(x, dropout=0.):
    """Dropout for sparse tensors."""
    dropout_mask = torch.floor(torch.rand(x._values().size())+(1-dropout)).type(torch.bool)
    i = x._indices()[:, dropout_mask]
    v = x._values()[dropout_mask]
    pre_out = torch.sparse.FloatTensor(i, v, x.shape)
    return pre_out * (1. / (1-dropout))

def sample_neg(net, val_ratio = 0.1, test_ratio=0.1, train_pos=None, test_pos=None, max_train_num=None):
    # get upper triangular matrix
    net_triu = ssp.triu(net, k=1)
    # sample positive links for train/test
    row, col, _ = ssp.find(net_triu)
    # sample positive links if not specified
    if train_pos is None or test_pos is None:
        perm = random.sample(list(range(len(row))), len(row))
        row, col = row[perm], col[perm]
        n_val = int(math.ceil(len(row) * val_ratio))
        n_test = int(math.ceil(len(row) * test_ratio))
        train_pos = (row[n_val+n_test:], col[n_val+n_test:])
        val_pos = (row[:n_val], col[:n_val])
        test_pos = (row[n_val:n_val+n_test], col[n_val:n_val+n_test])
    # if max_train_num is set, randomly sample train links
    if max_train_num is not None:
        perm = np.random.permutation(len(train_pos[0]))[:max_train_num]
        train_pos = (train_pos[0][perm], train_pos[1][perm])
    # sample negative links for train/test
    train_num, val_num, test_num = len(train_pos[0]), len(val_pos[0]), len(test_pos[0])
    neg = ([], [], [])
    n = net.shape[0]
    print('sampling negative links for train and test')
    while len(neg[0]) < train_num + test_num:
        i, j = random.randint(0, n-1), random.randint(0, n-1)
        if i < j and net[i, j] == 0:
            neg[0].append(i)
            neg[1].append(j)
        else:
            continue
    train_neg  = (neg[0][:train_num], neg[1][:train_num])
    test_neg = (neg[0][train_num:], neg[1][train_num:])
    return train_pos, train_neg, test_pos, test_neg

    
def links2subgraphs(A, train_pos, train_neg, test_pos, test_neg, h=2, max_nodes_per_hop=None, node_information=None):
    max_n_label = {'value': 0}
    def helper(A, links, g_label):
        g_list = []
        for i, j in tqdm(zip(links[0], links[1])):
            g, n_labels, n_features = subgraph_extraction_labeling((i, j), A, h, max_nodes_per_hop, node_information)
            max_n_label['value'] = max(max(n_labels), max_n_label['value'])
            g_list.append(GNNGraph(g, g_label, n_labels, n_features))
            del g, n_labels, n_features
        return g_list

        

    print('Enclosing subgraph extraction begins...')
    train_graphs = helper(A, train_pos, 1) + helper(A, train_neg, 0)
    test_graphs = helper(A, test_pos, 1) + helper(A, test_neg, 0)
    print(max_n_label)
    return train_graphs, test_graphs, max_n_label['value']

def parallel_worker(x):
    return subgraph_extraction_labeling(*x)
    
def subgraph_extraction_labeling(ind, A, h=2, max_nodes_per_hop=None, node_information=None):
    # extract the h-hop enclosing subgraph around link 'ind'
    dist = 0
    nodes = set([ind[0], ind[1]])
    visited = set([ind[0], ind[1]])
    fringe = set([ind[0], ind[1]])
    nodes_dist = [0, 0]
    for dist in range(1, h+1):
        fringe = neighbors(fringe, A)
        fringe = fringe - visited
        visited = visited.union(fringe)
        if max_nodes_per_hop is not None:
            if max_nodes_per_hop < len(fringe):
                fringe = random.sample(fringe, max_nodes_per_hop)
        if len(fringe) == 0:
            break
        nodes = nodes.union(fringe)
        nodes_dist += [dist] * len(fringe)
    # move target nodes to top
    nodes.remove(ind[0])
    nodes.remove(ind[1])
    nodes = [ind[0], ind[1]] + list(nodes) 
    subgraph = A[nodes, :][:, nodes]
    # apply node-labeling
    labels = node_label(subgraph)
    # get node features
    features = None
    if node_information is not None:
        features = node_information[nodes]
    # construct nx graph
    g = nx.from_scipy_sparse_array(subgraph)
    # remove link between target nodes
    if not g.has_edge(0, 1):
        g.add_edge(0, 1)
    return g, labels.tolist(), features


def neighbors(fringe, A):
    # find all 1-hop neighbors of nodes in fringe from A
    res = set()
    for node in fringe:
        nei, _, _ = ssp.find(A[:, node])
        nei = set(nei)
        res = res.union(nei)
    return res

def node_label(subgraph):
    # an implementation of the proposed double-radius node labeling (DRNL)
    K = subgraph.shape[0]
    subgraph_wo0 = subgraph[1:, 1:]
    subgraph_wo1 = subgraph[[0]+list(range(2, K)), :][:, [0]+list(range(2, K))]
    dist_to_0 = ssp.csgraph.shortest_path(subgraph_wo0, directed=False, unweighted=True)
    dist_to_0 = dist_to_0[1:, 0]
    dist_to_1 = ssp.csgraph.shortest_path(subgraph_wo1, directed=False, unweighted=True)
    dist_to_1 = dist_to_1[1:, 0]
    d = (dist_to_0 + dist_to_1).astype(int)
    d_over_2, d_mod_2 = np.divmod(d, 2)
    labels = 1 + np.minimum(dist_to_0, dist_to_1).astype(int) + d_over_2 * (d_over_2 + d_mod_2 - 1)
    labels = np.concatenate((np.array([1, 1]), labels))
    labels[np.isinf(labels)] = 0
    labels[labels>1e6] = 0  # set inf labels to 0
    labels[labels<-1e6] = 0  # set -inf labels to 0
    return labels

def AA(A, test_pos, test_neg):
    # Adamic-Adar score
    A_ = A / np.log(A.sum(axis=1))
    A_[np.isnan(A_)] = 0
    A_[np.isinf(A_)] = 0
    sim = A.dot(A_)
    return CalcAUC(sim, test_pos, test_neg)
    
        
def CN(A, test_pos, test_neg):
    # Common Neighbor score
    sim = A.dot(A)
    return CalcAUC(sim, test_pos, test_neg)


def CalcAUC(sim, test_pos, test_neg):
    pos_scores = np.asarray(sim[test_pos[0], test_pos[1]]).squeeze()
    neg_scores = np.asarray(sim[test_neg[0], test_neg[1]]).squeeze()
    scores = np.concatenate([pos_scores, neg_scores])
    labels = np.hstack([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])
    fpr, tpr, _ = metrics.roc_curve(labels, scores, pos_label=1)
    auc = metrics.auc(fpr, tpr)
    return auc

def single_line(batch_graphs):
    pbar = tqdm(batch_graphs, unit='iteration')
    graphs = []
    for graph in pbar:
        #line_graph, labels = to_line(graph, graph.node_tags)
        line_test(graph, graph.node_tags)
        #graphs.append(line_graph)
    return graphs

def gnn_to_line(batch_graph, max_n_label):
    start = time.time()
    pool = mp.Pool(16)
    #pool = mp.Pool(mp.cpu_count())
    results = pool.map_async(parallel_line_worker, [(graph, max_n_label) for graph in batch_graph])
    remaining = results._number_left
    pbar = tqdm(total=remaining)
    while True:
        pbar.update(remaining - results._number_left)
        if results.ready(): break
        remaining = results._number_left
        time.sleep(1)
    results = results.get()
    pool.close()
    pbar.close()
    g_list = [g for g in results]
    return g_list

def parallel_line_worker(x):
    return to_line(*x)

def to_line(graph, max_n_label):
    edges = graph.edge_pairs
    edge_feas = edge_fea(graph, max_n_label)/2
    edges, feas = to_undirect(edges, edge_feas)
    edges = torch.tensor(edges)
    data = Data(edge_index=edges, edge_attr=feas)
    data.num_nodes = graph.num_nodes
    data = LineGraph()(data)
    data.num_nodes = graph.num_edges
    data['y'] = torch.tensor([graph.label])
    return data

def to_edgepairs(graph):
    x, y = zip(*graph.edges())
    num_edges = len(x)
    edge_pairs = np.ndarray(shape=(num_edges, 2), dtype=np.int32)
    edge_pairs[:, 0] = x
    edge_pairs[:, 1] = y
    edge_pairs = edge_pairs.flatten()
    return edge_pairs

def to_linegraphs(batch_graphs, max_n_label):
    graphs = []
    pbar = tqdm(batch_graphs, unit='iteration')
    for graph in pbar:
        edges = graph.edge_pairs
        edge_feas = edge_fea(graph, max_n_label)/2
        edges, feas = to_undirect(edges, edge_feas)
        del edge_feas
        edges = torch.tensor(edges)
        data = Data(edge_index=edges, edge_attr=feas)
        del edges, feas
        data.num_nodes = graph.num_nodes
        data = LineGraph()(data)
        data['y'] = torch.tensor([graph.label])
        data.num_nodes = graph.num_edges
        graphs.append(data)
        del data
    return graphs

def edge_fea(graph, max_n_label):
    node_tag = torch.zeros(graph.num_nodes, max_n_label+1)
    tags = graph.node_tags
    tags = torch.LongTensor(tags).view(-1,1)
    node_tag.scatter_(1, tags, 1)
    return node_tag

def edge_fea2(labels, edges):
    feas = []
    for i in range(edges.shape[1]):
        fea = [labels[edges[0][i]], labels[edges[1][i]]]
        fea.sort()
        feas.append(fea)
    feas = np.reshape(feas, [-1, 2])
    feas = np.array([feas[:,0], feas[:,1]], dtype=np.float32)
    return torch.tensor(feas/2)
    
def to_undirect2(edges):
    edges = np.reshape(edges, (-1,2 ))
    sr = np.array([edges[:,0], edges[:,1]], dtype=np.int64)
    rs = np.array([edges[:,1], edges[:,0]], dtype=np.int64)
    target_edge = np.array([[0,1],[1,0]])
    return np.concatenate([target_edge, sr, rs], axis=1)
    
def to_undirect(edges, edge_fea):
    edges = np.reshape(edges, (-1,2 ))
    sr = np.array([edges[:,0], edges[:,1]], dtype=np.int64)
    fea_s = edge_fea[sr[0,:], :]
    fea_s = fea_s.repeat(2,1)
    fea_r = edge_fea[sr[1,:], :]
    fea_r = fea_r.repeat(2,1)
    fea_body = torch.cat([fea_s, fea_r], 1)
    rs = np.array([edges[:,1], edges[:,0]], dtype=np.int64)
    return np.concatenate([sr, rs], axis=1), fea_body


def line_test(graph, label):
    edges = graph.edge_pairs
    edges= to_undirect2(edges)
    feas = edge_fea2(label, edges)
    data = Data(edge_index=torch.tensor(edges), edge_attr=feas.T)
    data = LineGraph()(data)
    elist = data['edge_index'].numpy()